TITLE: Solving SMR with Crash Faults in Partial Synchrony (Essence of Paxos & Raft)

OVERVIEW:
- This lecture tackles State Machine Replication (SMR) under crash faults in a partially synchronous network.
- We learn:
  1. The partial synchrony model, a practical middle ground between fully synchronous and fully asynchronous models.
  2. Why SMR with crash faults under partial synchrony requires a strict majority of non-faulty validators.
  3. The essence of the Paxos/Raft protocol—widely used in production systems—to achieve consistency and liveness when less than half the validators can fail.

--------------------------------------------------
1. RECAP: SMR AND THE ROAD MAP
- State Machine Replication (SMR):  
  - Each validator maintains an append-only list (chain) of finalized transactions.  
  - The protocol must guarantee consistency (no validator disagrees on the chain) and liveness (every valid transaction is eventually added).
- Road Map:  
  - Lecture 3 showed how to solve SMR with crash faults in a fully synchronous model (Protocol B).  
  - FLP Theorem: No deterministic algorithm can guarantee SMR in a fully asynchronous network if just one validator can crash.  
  - Partial synchrony: This lecture's focus. In partial synchrony, the network may behave asynchronously for some unknown time but eventually behaves synchronously.

--------------------------------------------------
2. PARTIAL SYNCHRONY MODEL
- Intuition: We want to accommodate temporary outages/attacks (unbounded message delays for a while), but eventually the network "recovers" so messages arrive within a known bound.
- Formal Model:
  - There is a global clock in discrete timesteps (0,1,2,...).
  - A known upper bound Δ on message delay after the system stabilizes.
  - A (hidden) Global Stabilization Time (GST): Before GST, message delays can be arbitrarily large; after GST, all messages arrive within Δ timesteps.
  - Key requirement: The protocol must work correctly no matter how long it takes to reach GST, and once GST occurs, the protocol guarantees liveness.

--------------------------------------------------
3. SECURITY THRESHOLDS
- A security threshold is the fraction of faulty validators at which consensus flips from possible to impossible.
- Crash faults + synchronous network: threshold ~100%. (Protocol B remains correct if only one validator is still alive.)
- Crash faults + asynchronous network: threshold ~0%. (With FLP, a single crash fault can derail consensus.)
- Crash faults + partial synchrony: threshold < 50%.  
  - We need a strict majority of non-faulty validators to distinguish genuine crashes from just delayed messages.
  - Example scenario: If half the validators are in one partition and half in another, each partition is unsure if the other side has crashed or if messages are just delayed.

--------------------------------------------------
4. PROTOCOL C (PAXOS/RAFT ESSENCE)
- High-Level Idea:
  1. Time is divided into "views." Each view has a designated leader who tries to propose a new block of transactions.
  2. Validators form read quorums (leaders gather information from > n/2 validators) and write quorums (a block is finalized if acknowledged by > n/2 validators).
  3. Intersection of quorums ensures any finalized proposal in one view is extended (not forked) by any future proposal.

- Key Design Patterns:
  - (a) Leader-based: Only one validator proposes a new block in a given view.  
  - (b) Quorum intersection: More than half the validators needed for both read and write phases ensures overlap.
  - (c) Catch-up phase: Leader collects the most up-to-date chains from a read quorum before proposing.  
  - (d) Simultaneous updates: Validators finalize a block only if they see a majority "ack" from the write quorum.

### 4.1 Protocol C Details
- View = 3Δ timesteps (phases of length Δ each):
  1. Catch-Up (0 to Δ):  
     - Every validator i sends its current chain A_i to the leader of view v.
  2. Propose (Δ to 2Δ):  
     - Leader waits to receive more than n/2 (read quorum) chains A_i.  
     - Leader picks the chain with the highest view number (most recent) from these chains, extends it with any new transactions, and sends the proposal A* back to all validators.
  3. Acknowledge & Finalize (2Δ to 3Δ):  
     - Any validator receiving A* sends an "ack" to all other validators. It also updates its local chain A_i to A*.  
     - If a validator sees more than n/2 "ack" messages for A*, it finalizes A* (updates its finalized chain C_i).

### 4.2 Consistency Proof
- Key Claim: If some validators finalize A* in view v, then any proposal finalized in a later view v' (where v' > v) extends A*.  
  - Reason: Quorum intersection. Any new leader in view v'+1 gathers read quorum more than n/2, which overlaps with the more than n/2 that acknowledged A*. Thus, the leader learns about A* and extends it, avoiding forks.

### 4.3 Liveness Proof
- Once the network stabilizes after GST, the leader of the next view that starts post-GST can gather all information from non-faulty nodes and propose a block.  
- That proposal will be seen by more than n/2 validators, who will "ack" it, so it becomes finalized.  
- Therefore, any transaction known by a non-faulty validator will eventually be included in a block in some post-GST view.

--------------------------------------------------
5. TAKEAWAYS
1. Partial synchrony is a sweet spot:  
   - Temporarily tolerates unbounded delays (pre-GST), yet once the network stabilizes, consensus proceeds at a known pace (∆-bounded delays).
2. Majority fault tolerance: More than half the validators must be correct to guarantee consistency.
3. Paxos/Raft-style protocols use read and write quorums to ensure consistency and liveness, given partial synchrony and a majority of non-faulty validators.
4. Implementation & Industry Use:  
   - Variants of Paxos (like Raft) are popular in distributed databases and enterprise systems, illustrating the viability of SMR under crash faults in partial synchrony.

--------------------------------------------------
END OF LECTURE 4 NOTES